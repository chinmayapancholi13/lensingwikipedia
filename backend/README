Backend stuff, including the index builder.

Usage and introduction
======================

Creating an index from the data
-------------------------------

The first step is putting the basic data from the data preparation step into a Whoosh index. A Whoosh index is just a directory that Whoosh keeps its files in. The input to this step is a data file where each line of the file is a JSON object for a single event.

Let's say that the data file is called data.json and we want to put it in a new Whoosh index called data.index. First we index the base data:

	buildindex data.index data.json

You can also omit the Json file name and send the data on standard input instead.

Next we need to assign references points to all events, which we do with the cluster program:

	cluster data.index

This updates the index in-place. See the usage for other options.

Starting the backend
--------------------

To minimally start a backend, simply give the path to the index on the command line:

	backend -i data.index

If you need to change the port, use -p. However, with the above invocation there is no way to change the path to the index or other settings without restarting the backend. The backend can instead be given a configuration file to read settings from. Let's say that we put the following in demo.conf:

	{
	  'server': {
	    'index_dir_path': 'data.index'
	  },
	}

We can then start a backend using this configuration file:

	backend -c demo.conf

Note that the index path is relative to wherever the backend is started. Setting an absolute path may be safer.

Backend behaviour and configuration
-----------------------------------

The backend reloads the configuration file and updates its settings at an interval determined by the settings_timeout server setting. If there is any error reading the configuration file, the backend ignores the file and continues using the current settings.

Note that if you change the configuration file non-atomically, there is the possibility that the backend will see incorrect or unreadable settings. To avoid this, you could for example create a separate new file and use unix mv to replace the configuration file atomically.

When the backend reloads the configuration file, it also checks for changes to the index. If the index path has not changed but the index has been modified since the last settings reload, the backend resets by clearing all caches and re-priming them. If the index path (given by the index_dir_path server setting) has changed, it starts using the new index and then does the same reset. If you want to force the backend to reset at every settings reload regardless of index changes you can use the always_reset server setting (but there is no obvious reason to do so).

The settings reload and any necessary index loading and cache priming is done in a background thread. The changes will be applied at the next query received after all such work is done. Any query received while the reload and reset is still running will be handled with the old settings, index, and caches.

Note that the configuration file is configuration for a backend instance, not configuration for the data. In general each live instance of the backend should have its own configuration file so that its settings can be changed without restarting.

See default_settings.py for a list of settings and the format of a configuration file. In general you would want to at least set the index path and make sure that the all_argument_numbers and fields_to_prime querier settings match your data.

Testing queries
---------------

For testing, we can run a query directly against the index (without the backend, but using the same query handling code):

        queryindex -c demo.conf test.index query.json

The -c option here uses the querier settings (but not server settings) from test.conf.

We can also run a query through a running backend:

        querybackend http://localhost:1500 query.json

You can try this with the .json query files in examplequeries/, for example.

Index format
============

Event data
----------

Only the data needed to support the backend protocol is indexed. For details see the buildindex program, especially make_event_doc().

The principles used in preparing data for the index are:
- Any text that needs to be pre-formatted is added as a new field.
- Relevant information from fields with compound information is extracted into flat lists for efficient querying.
- If a set of fields will be searched on together, a new field is added to combine them.

We use Whoosh not only to index the data for searching but also to retrieve complete results. Thus fields should have the stored flag set in the Whoosh schema unless they are strictly not needed in results.

Numbers can be given to Whoosh as numbers (rather than strings). All strings must be given as unicode. Whoosh requires the fields containing a list of keywords be given as strings with either space or comma separators. Code in whooshutils.py is responsible for standardizing the conversion to this format, including escaping any separator characters in the input.

Keywords may have whitespace normalized at the indexing step, in the interests of easy and consistent list formatting.

Reference points
----------------

Reference points are assigned to each event based on the event's geographic coordinates. These are currently produced with a clustering algorithm, but they could also be produced by eg rounding coordinates or snapping to some grid. What constitutes an appropriate choice of reference points is determined by how the frontend uses them. Reference points are longitude-latitude points as strings with the two coordinates separated by a comma.

Queries
=======

A query is a JSON object with the following format:
	{
		"constraints": /* dictionary of constraints keyed by ID */,
		"views": /* dictionary of views keyed by ID */
	}

A response is a dictionary of results keyed by view ID. The value for each ID is the result for the corresponding view. Each result is a JSON structure, with the details depending on the view. The results are based only on the events in the data which match all of the constraints.

Years are given as integers, where negative indicates BCE and positive indicates CE.

Some view types are paginated (see the code and the design notes below for details) or can optionally be paginated. These views have a page attribute which sets the page number, starting from zero. The number of items per page is determined by the backend. The result contains details for this number of events, or as many are available to match the constraints if less than a full page is available. The result also has a "more" attribute indicating whether more pages are available. Submitting a query with the same constraints and the same view except for a changed page number will fetch subsequent pages. If a view type is strictly paginated then it will assume a page number of zero if no page number is given. If it is optionally paginated it will paginate only if a page number is given, and otherwise will return all possible results. If a page number past the last page is requested then the result will be properly structured and have the "more" attribute set to false, but the contents are otherwise undefined.

Any view result can have an error; see below.

See examplequeries/ for examples.

Constraints
-----------

For details see queries.py, especially constraint_to_sdb_query().

### Constrain to events where a field has a particular value:
	{
		"type": "fieldvalue",
		"field": /* name of field to match */,
		"value": /* value for the field to match */
	}

### Constrain to events in an time range (inclusive):
	{
		"type": "timerange",
		"low": /* lower bound year (inclusive, and see year format above) */,
		"high": /* upper bound year (inclusive, and see year format above) */
	}

### Constrain to events in particular reference points:
	{
		"type": "referencepoints",
		"points": /* list of reference points to match */
	}

Views and results
-----------------

For details see queries.py, especially generate_views().

### Get counts of events by value of a particular field:
	{
		"type": "countbyfieldvalue",
		"field": /* name of field to count on */,
		"page": /* optional; page number, defaults to 0 if not given */
	}
result:
	{
		"counts": /* dictionary of count pairs, each of which is a value and an integer count */,
		"more": /* boolean flag indicating if there are more pages available */
	}
This view is always paginated.

### Get counts of events by map reference point:
	{
		"type": "countbyreferencepoint",
		"page": /* optional; page number, no pagination if not given */
	}
result:
	{
		"counts": /* dictionary of count pairs, each of which is a value and an integer count */,
		"more": /* boolean flag indicating if there are more pages available, set only if pagination is enabled */
	}
This view is optionally paginated.

### Get counts of events by year:
	{
		"type": "countbyyear",
		"page": /* optional; page number, no pagination if not given */
	}
result:
	{
		"counts": /* dictionary of count pairs, each of which is a value and an integer count */,
		"more": /* boolean flag indicating if there are more pages available, set only if pagination is enabled */
	}
This view is optionally paginated.

### Get descriptive details of events:
	{
		"type": "descriptions",
		"page": /* optional; page number, defaults to 0 if not given */
	}
result:
	{
		"descriptions": /* list of event details */,
		"more": /* boolean flag indicating if there are more pages available for this view */
	}
where each element corresponds to a particular event and has the format:
	{
		"year": /* year (see year format above) */,
		"descriptionHtml": /* HTML-formatted description */
	}
This view is always paginated.

Errors
------

Any view result can be replaced by an error if an error occurred when handling that view. The error could be intended if the backend refuses to handle a certain constraint, or could indicate an internal backend error. An error result has the format:
	{
		"error": /* true boolean error message string */
	}
The value of the error attribute is either a boolean true value if there is no specific error message, or a string containing a specific error message.

Design notes
============

Miscellaneous 
-------------

The backend is designed to have no per-client state. Caching is used in limited cases where being stateless is a potential performance issue.

We cache all initial (empty constraint) view results. The predicate should_cache() in queries.py determines which views need result caching, so the scope of this caching can be expanded easily if desired.

All caches are cleared at the timeout when the backend reloads its settings if there has been any change to the index (identity or contents), so that if the data changed we get the current version. The caches are initially primed by auto-submitting common queries. The queries_to_prime() function in queries.py generates the queries for priming.

Caches are controlled by being isolated in a query handler (Querier in queries.py) object. This allows a complete reset of the query handling state (for resetting caches) by creating a new query handler.

The backend uses a single background thread which handles settings loading, index loading, and creating new query handlers (to reset the caches), and waits on the timeout between settings loads. This thread sends settings and query handler changes to the main thread through a thread-safe queue.

Pagination
----------

For pagination of count view results, we get complete results (without pagination), sort by the count, and clip to the desired page size. We cache the unclipped results so that this isn't horribly inefficient. It should be efficient enough as long as the cache is large enough relative to the number of users. Which views get this kind of pagination is controlled by how_to_paginate_results() in queries.py.

For queries which don't need counting, Whoosh handles pagination for us through its search_page() function. The Whoosh documentation indicates that this function simply runs the complete query but only returns the appropriate page (presumably similar to our clipping on counted results), but in practice this seems incur little slowdown for larger page numbers.
