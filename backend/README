Backend stuff, including the uploader.

Usage and introduction
======================

Uploading data
--------------

The first step is putting the basic data from the data preparation step into the database. The input is a local data file where each line of the file is a JSON object for a single event.

Let's say that the local data file is called data.json and we want to put it in a SimpleDB domain called lensingwikipedia-demo-data. First we do the main upload:
        upload -d lensingwikipedia-demo-data data.json
The -d option makes upload delete any existing domain with the name and replace it with the new one.

Next we need to assign references points to all events, which we do with the cluster program:
	cluster lensingwikipedia-demo-data data.json

Events in the database are identified by integers which are the indices in the input data. The intention is that any upload will replace a domain or create a new one. It is possible to change or add to an existing domain using upload -a (and possibly -i to control the initial index), but this must be done carefully so that the indices match up with what is already in the domain. Similarly the indices to the cluster program (also controlled by -i) must match up.

You can remove the data by removing the domain, such as with the removedomain program. Do so with caution.

Starting the backend
--------------------

The backend stores its settings in another SimpleDB domain, let's say lensingwikipedia-demo-backend-settings. The only setting that doesn't have a default is the name of the data domain:
        backend-ctrl --data_domain_name=lensingwikipedia-demo-data lensingwikipedia-demo-backend-settings
You will also likely want to set fields_to_prime so that caches can be primed in advance (see below).

Once these settings are set, then we can run the backend on this settings domain.
        backend lensingwikipedia-demo-backend-settings
If you need to change the port, use -p.

Note that the settings domain is an IPC mechanism for controlling the backend at runtime, not a configuration file associated with the data. In general each instance of the backend should have its own settings domain.

Backend behaviour and configuration
-----------------------------------

The backend loads the settings from the settings domain when it starts, and reloads them each time a timeout period expires. The timeout period is also loaded as a setting and thus can be changed at runtime.

After the settings are loaded, the backend may also reset its query handling and clear all caches (thereby ensuring that any new data in the data domain is reflected in responses to queries). This occurs if one of the following is the case:
- The name of the data domain has changed.
- The setting reset_always is set to true.
- The setting reset_next is set to true.
The distinction between reset_always and reset_next is that the former is a regular setting, while the later is a special flag which the backend reads and then resets to false if it was true. Thus the former indicates that the caches will always be reset on a settings load, while the later indicates that the caches will be reset on the next settings load but not on future settings loads unless requested again.

After clearing the caches the backend primes them with commonly accessed queries. The _to_prime settings control which views are primed.

Settings loading and cache priming are done in a background thread so that the backend can (in the main thread) always respond to queries immediately. The change to new settings and new caches occurs on the first request after both are available and ready to use.

The backend settings are:
	data_domain_name -- name of the data domain
	settings_timeout -- timeout between settings reloads, in seconds
	reset_always -- always clear caches after loading settings
	reset_next -- clear caches after the next settings load (special flag, see above)
	description_page_size -- number of events in a page of description results
	count_by_field_value_page_size -- number of events in a page of count by field value results
	count_by_year_page_size -- number of events in a page of count by year results
	count_by_referencepoint_page_size -- number of events in a page of count by reference point results
	num_initial_description_pages_to_cache -- number of pages of description results for the initial (empty) view to cache
	query_pagination_cache_size -- number of SimpleDB next pointers to store for results which are paginated by the databse
	result_pagination_cache_size -- number of results to cache for results which are paginated by the backend
	all_argument_numbers -- list of all role arguments, given to backend-ctrl as a comma-separated list
	fields_to_prime -- list of all field names to prime the caches with, given to backend-ctrl as a comma-separated list

See backend_settings.py for the defaults defaults, which are used if a setting is not set in the settings domain. To unset a setting (and therefore revert to the default), give it an empty value, eg:
	backend-ctrl --description_page_size= lensingwikipedia-demo-backend-setting
backend-ctrl can also input and output settings in a text format. See the program for more information.

Testing queries
---------------

For testing, we can run a query directly against the database (without the backend, but using the same code):
        querydb lensingwikipedia-demo-data query.json
Or we can run a query through the backend:
        querybackend http://localhost:1500 query.json

You can try this with the .json files in examplequeries/, for example.

Database format
===============

Event data
----------

The input data is uploaded to the data domain of the database as literally as possible. For details see the upload program source, especially prepare_event().

The principles used in preparing data for the database are:
- Any text that needs to be pre-formatted is added as a new field.
- Any field that needs to be sorted on numerically is normalized and added as a new field. SimpleDB supports only lexicographical sorting, so normalization must pad and shift to non-negative values as necessary to make lexicographical sorting equivalent to numeric sorting.
- All long text fields are clipped to fit in the SimpleDB value size limit.
- Fields which have compound information (ie are not a simple value or list of simple values) are flatted by packing things into strings or lists of strings.
- Relevant information from fields with compound information is extracted into flat lists for efficient querying.

Reference points
----------------

Reference points are assigned to each event based on the event's geographic coordinates. These are currently produced with a clustering algorithm, but they could also be produced by eg rounding coordinates or snapping to some grid. What constitutes an appropriate choice of reference points is determined by how the frontend uses them. Reference points are longitude-latitude points as strings with the two coordinates separated by a comma.

Queries
=======

A query is a JSON object with the following format:
	{
		"constraints": /* dictionary of constraints keyed by ID */,
		"views": /* dictionary of views keyed by ID */
	}

A response is a dictionary of results keyed by view ID. The value for each ID is the result for the corresponding view. Each result is a JSON structure, with the details depending on the view. The results are based only on the events in the database which match all of the constraints.

Years are given as integers, where negative indicates BCE and positive indicates CE.

Some view types are paginated (see the code and the design notes below for details) or can optionally be paginated. These views have a page attribute which sets the page number, starting from zero. The number of items per page is determined by the backend. The result contains details for this number of events, or as many are available to match the constraints if less than a full page is available. The result also has a "more" attribute indicating whether more pages are available. Submitting a query with the same constraints and the same view except for a changed page number will fetch subsequent pages. If a view type is strictly paginated then it will assume a page number of zero if no page number is given. If it is optionally paginated it will paginate only if a page number is given, and otherwise will return all possible results.

See examplequeries/ for examples.

Constraints
-----------

For details see queries.py, especially constraint_to_sdb_query().

### Constrain to events where a field has a particular value:
	{
		"type": "fieldvalue",
		"field": /* name of field to match */,
		"value": /* value for the field to match */
	}

### Constrain to events in an time range (inclusive):
	{
		"type": "timerange",
		"low": /* lower bound year (inclusive, and see year format above) */,
		"high": /* upper bound year (inclusive, and see year format above) */
	}

### Constrain to events in particular reference points:
	{
		"type": "referencepoints",
		"points": /* list of reference points to match */
	}

Views and results
-----------------

For details see queries.py, especially generate_views().

### Get counts of events by value of a particular field:
	{
		"type": "countbyfieldvalue",
		"field": /* name of field to count on */,
		"page": /* optional; page number, defaults to 0 if not given */
	}
result:
	{
		"counts": /* dictionary of count pairs, each of which is a value and an integer count */,
		"more": /* boolean flag indicating if there are more pages available */
	}
This view is always paginated.

### Get counts of events by map reference point:
	{
		"type": "countbyreferencepoint",
		"page": /* optional; page number, no pagination if not given */
	}
result:
	{
		"counts": /* dictionary of count pairs, each of which is a value and an integer count */,
		"more": /* boolean flag indicating if there are more pages available, set only if pagination is enabled */
	}
This view is optionally paginated.

### Get counts of events by year:
	{
		"type": "countbyyear",
		"page": /* optional; page number, no pagination if not given */
	}
result:
	{
		"counts": /* dictionary of count pairs, each of which is a value and an integer count */,
		"more": /* boolean flag indicating if there are more pages available, set only if pagination is enabled */
	}
This view is optionally paginated.

### Get descriptive details of events:
	{
		"type": "descriptions",
		"page": /* optional; page number, defaults to 0 if not given */
	}
result:
	{
		"descriptions": /* list of event details */,
		"more": /* boolean flag indicating if there are more pages available for this view */
	}
where each element corresponds to a particular event and has the format:
	{
		"year": /* year (see year format above) */,
		"descriptionHtml": /* HTML-formatted description */
	}
This view is always paginated.

Other design notes
==================

Miscellaneous 
-------------

The backend is designed to have no per-client state. Caching is used in limited cases where being stateless is a potential performance issue.

We cache all initial (empty constraint) view results. The predicate should_cache() in queries.py determines which views need result caching, so the scope of this caching can be expanded easily if desired.

All caches are cleared at the timeout when the backend reloads its settings, so that if the data changed we get the current version. The caches are initially primed by auto-submitting common queries. The queries_to_prime() function in queries.py generates the queries for priming.

Caches are controlled by being isolated in a query handler (Querier in queries.py) object. This allows a complete reset of the query handling state (for resetting caches) by creating a new query handler.

The backend uses a single background thread which handles settings loading, creating new query handlers (to reset the caches), and waits on the timeout between settings loads. This thread sends settings and query handler changes to the main thread through a thread-safe queue.

Pagination
----------

For some queries (the descriptions view) we can have SimpleDB do pagination for us. Pagination directly to SimpleDB is easy, since SimpleDB queries return a next pointer which can be used to continue the query. However, providing this through the backend without identifying clients is not as trivial. The solution involves counting to skip ahead to the desired starting point in a query, but caching next points so that this is generally not necessary. See sdbutils.py for details and reference(s). In the code this type of pagination tends to be called "query pagination".

For other queries (all the count views) there is no way to have SimpleDB do the pagination. For these views we get all results from the DB, do counting, and clip to the desired page size. We cache the unclipped results so that this isn't horribly inefficient. It should be efficient enough as long as the cache is large enough relative to the number of users. In the code this type of pagination tends to be called "result pagination".

Duplicated items in SimpleDB query results
------------------------------------------

SimpleDB has an interesting behaviour where it sometimes decides to return some items more than once in the same query results. For example, say I run these to queries which differ only by one condition:
        dom.select("select `personText`,`referencepoints`,`year` from `test1` where ((`personText` is not null) or (`referencepoints` is not null))")
        dom.select("select `personText`,`referencepoints`,`year` from `test1` where ((`personText` is not null) or (`referencepoints` is not null) or (`year` is not null))")
The result for the first query won't have any duplicate items, but the result for the second will. This item shows up once for the first query and twice for the second, for example:
        {u'personText': u'Vidkun Quisling', u'referencepoints': u'5.962572,46.935778', u'year': u'1945'}
I believe that duplicated items are expected in the case of sorting on a multiple-valued attribute, but that is not the case here.

Because of this behaviour, for non-paginated DB queries our DB query sending code in sdbutils.py keeps track of which items it has seen and ensures that each is visited only once. However, this isn't done for results which are paginated by the DB since it would be more complicated to get consistent results there and our DB paginated queries don't seem to have this behaviour. Result pagination on the backend has the fix.

Checking for duplicated items when receiving them may be inefficient. It would perhaps be better to only do this on queries that really need it. Multiple items seem to occur particularly in queries with several "is not null" conditions (as in the above example), which we currently use for counting by value. An alternative would be to remove all "is not null" conditions from such queries. The DB would then return some items which are irrelevant (in that they don't have any of the fields we are counting on), but it isn't clear that this is worse for either bandwidth or processing time then the DB sending some duplicated items. Additionally, we may be able to reasonably expect that most items will have the fields.

Predicate and value tests per predicate limits in SimpleDB
----------------------------------------------------------

SimpleDB allows somewhere around 20 predicates and somewhere around 20 value tests (condition operators) per predicate (I'm not sure where this is in official documentation, but see https://forums.aws.amazon.com/thread.jspa?threadID=4043 and http://en.wikipedia.org/wiki/Amazon_SimpleDB#Query_limitations). A predicate is the set of checks against one attribute (DB field) within each term of an intersection, or the whole query if there are no intersections. A predicate also seems to be broken by having a condition on a different attribute between two conditions against one attribute, although I again haven't found specific documentation on this. Therefore it appears that any consecutive checks (which may have internal parenthesis groupings) against the same attribute.

This poses a problem for constraints on reference point, which require a long list of reference points selected as an "in" condition. If there are more than 20 points, we can use a dummy field trick (see http://blog.yslin.tw/2012/03/simpledb-too-many-value-tests-per.html) to get around the value tests per predicate limit. However, this increases the number of predicates.

The only thing that I see which would fully solve the issue is having the DB query handling code automatically submit multiple queries to handle the disjunction if needed. As the map view may change I don't see any reason to implement this until it is definitely required, so the current implementation uses only the dummy field trick. An intermediate trick would be to duplicate the reference points field so that it is possible to do multiple "in" conditions (against separate attributes) without requiring the extra conditions on dummy fields, which generate extra predicates.
